"""
=============================================
âš™ï¸ æ ¸å¿ƒä¸šåŠ¡é€»è¾‘æ¨¡å—
=============================================
æ¨¡å—åç§°: services.py
æ¨¡å—åŠŸèƒ½:
    - æ–‡ä»¶ä¸Šä¼ å¤„ç†: æ ¡éªŒ -> æŸ¥é‡ -> å‹ç¼© -> åŠ å¯† -> å­˜å‚¨
    - æ–‡ä»¶è¯»å–å¤„ç†: è¯»å– -> è§£å¯† -> è§£å‹ -> è¿”å›
    - åå°æ¸…ç†ä»»åŠ¡: å®šæœŸæ¸…ç†è¿‡æœŸæ–‡ä»¶
    - TTL ç¼“å­˜: æ–‡ä»¶å…ƒæ•°æ®ç¼“å­˜ï¼ˆ5åˆ†é’Ÿè¿‡æœŸï¼‰
æ•°æ®å¤„ç†æµç¨‹:
    å†™å…¥: æ¥æ”¶æ–‡ä»¶ -> JSON æ ¡éªŒ -> BLAKE2b å“ˆå¸Œ -> å»é‡æ£€æŸ¥ -> å‹ç¼© -> åŠ å¯† -> å­˜å‚¨
    è¯»å–: è¯»å–æ–‡ä»¶ -> è§£å¯† -> è§£å‹ -> è¿”å› JSON

ä½¿ç”¨çš„ Python æ ‡å‡†åº“æ¨¡å—:
    - pathlib.Path: ç°ä»£è·¯å¾„æ“ä½œ
    - secrets: å®‰å…¨éšæœºæ•°ç”Ÿæˆï¼ˆæ–‡ä»¶ IDï¼‰
    - hashlib.blake2b: é«˜é€Ÿå“ˆå¸Œè®¡ç®—
    - functools.cached_property: å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆconfig.pyï¼‰
    - contextlib.asynccontextmanager: å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆdatabase.pyï¼‰

"""

# ========== æ ‡å‡†åº“å¯¼å…¥ ==========
import hashlib  # å“ˆå¸Œè®¡ç®—
import gzip  # Gzip å‹ç¼©
import secrets  # å®‰å…¨éšæœºæ•°ç”Ÿæˆ
import datetime  # æ—¶é—´å¤„ç†
import asyncio  # å¼‚æ­¥ä»»åŠ¡
import re  # æ­£åˆ™è¡¨è¾¾å¼
import time  # æ—¶é—´æˆ³
import psutil  # ç³»ç»Ÿä¿¡æ¯
from pathlib import Path  # è·¯å¾„æ“ä½œ

# ========== ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ ==========
import anyio  # å¼‚æ­¥æ–‡ä»¶æ“ä½œ
import orjson  # é«˜æ€§èƒ½ JSON å¤„ç†
from fastapi import UploadFile, HTTPException
from dataclasses import dataclass
from typing import Any
from cachetools import TTLCache  # TTL ç¼“å­˜

# ========== å†…éƒ¨æ¨¡å—å¯¼å…¥ ==========
from app.core.config import Config
from app.database import get_db_connection
from app.models import TimeLimit
from app.core.logger import log
from app.core.http_client import http_client
from app.core.crypto import CryptoEngine


# ==========================================
# âš™ï¸ JSON éªŒè¯é…ç½®
# ==========================================

@dataclass
class JSONValidationConfig:
    """JSON éªŒè¯é…ç½®ç±»"""
    max_depth: int = 20        # æœ€å¤§åµŒå¥—æ·±åº¦
    max_fields: int = 1000     # æœ€å¤§å­—æ®µ/æ•°ç»„é•¿åº¦
    max_total_length: int = 10 * 1024 * 1024  # æœ€å¤§æ€»å¤§å° (10MB)


def _validate_json_structure(obj: Any, depth: int = 0, config: JSONValidationConfig | None = None) -> None:
    """
    é€’å½’éªŒè¯ JSON ç»“æ„

    é˜²æ­¢æ·±åº¦åµŒå¥—æ”»å‡»å’Œè¶…å¤§å¯¹è±¡æ”»å‡»

    Args:
        obj: å¾…éªŒè¯çš„ JSON å¯¹è±¡
        depth: å½“å‰åµŒå¥—æ·±åº¦
        config: éªŒè¯é…ç½®

    Raises:
        HTTPException: éªŒè¯å¤±è´¥æ—¶æŠ›å‡º
    """
    if config is None:
        config = JSONValidationConfig()

    # æ£€æŸ¥æ·±åº¦
    if depth > config.max_depth:
        raise HTTPException(
            status_code=400,
            detail=f"ğŸ“„ JSON åµŒå¥—è¿‡æ·±ï¼ˆæœ€å¤§ {config.max_depth} å±‚ï¼‰"
        )

    # æ£€æŸ¥å­—æ®µæ•°é‡
    if isinstance(obj, dict):
        if len(obj) > config.max_fields:
            raise HTTPException(
                status_code=400,
                detail=f"ğŸ“„ JSON å­—æ®µè¿‡å¤šï¼ˆæœ€å¤§ {config.max_fields} ä¸ªï¼‰"
            )
        for value in obj.values():
            _validate_json_structure(value, depth + 1, config)
    elif isinstance(obj, list):
        if len(obj) > config.max_fields:
            raise HTTPException(
                status_code=400,
                detail=f"ğŸ“„ JSON æ•°ç»„è¿‡é•¿ï¼ˆæœ€å¤§ {config.max_fields} ä¸ªå…ƒç´ ï¼‰"
            )
        for item in obj:
            _validate_json_structure(item, depth + 1, config)


# ==========================================
# ğŸ’¾ TTL ç¼“å­˜
# ==========================================

# å…¨å±€ç¼“å­˜ï¼šæ–‡ä»¶å…ƒæ•°æ®ï¼ˆ5åˆ†é’Ÿè¿‡æœŸï¼‰
_metadata_cache: TTLCache = TTLCache(maxsize=2048, ttl=300)

# å…¨å±€ç¼“å­˜ï¼šå“ˆå¸ŒæŸ¥é‡ç»“æœï¼ˆ1åˆ†é’Ÿè¿‡æœŸï¼‰
_hash_cache: TTLCache = TTLCache(maxsize=4096, ttl=60)


def invalidate_file_cache(file_id: str) -> None:
    """
    ğŸ—‘ï¸ æ¸…é™¤æ–‡ä»¶ç¼“å­˜

    Args:
        file_id: æ–‡ä»¶ ID
    """
    _metadata_cache.pop(file_id, None)


# ==========================================
# ğŸ”§ å·¥å…·å‡½æ•°
# ==========================================

def compress_data(data: bytes) -> bytes:
    """
    ğŸ—œï¸ å‹ç¼©æ•°æ®

    ä½¿ç”¨ Gzip ç®—æ³•å‹ç¼©æ•°æ®ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´å’Œå¸¦å®½

    Args:
        data: å¾…å‹ç¼©çš„åŸå§‹å­—èŠ‚æ•°æ®

    Returns:
        bytes: å‹ç¼©åçš„æ•°æ® (æœªå¯ç”¨å‹ç¼©æ—¶è¿”å›åŸæ•°æ®)

    æ³¨æ„:
        - å‹ç¼©ç­‰çº§ç”± COMPRESSION_LEVEL æ§åˆ¶ (1-9)
        - å…¸å‹ JSON æ–‡ä»¶å¯å‹ç¼© 60-80%
    """
    if Config.COMPRESSION_ENABLED:
        return gzip.compress(data, compresslevel=Config.COMPRESSION_LEVEL)
    return data


def decompress_data(data: bytes) -> bytes:
    """
    ğŸ“¦ è§£å‹æ•°æ®

    ä½¿ç”¨ Gzip ç®—æ³•è§£å‹æ•°æ®

    Args:
        data: å¾…è§£å‹çš„å­—èŠ‚æ•°æ®

    Returns:
        bytes: è§£å‹åçš„åŸå§‹æ•°æ®

    æ³¨æ„:
        - è‡ªåŠ¨æ£€æµ‹æ•°æ®æ˜¯å¦ä¸º Gzip æ ¼å¼ (é­”æ•°: 0x1f 0x8b)
        - éå‹ç¼©æ•°æ®ç›´æ¥è¿”å›åŸæ ·
    """
    # æ£€æŸ¥æ˜¯å¦ä¸º Gzip æ ¼å¼ (é­”æ•°æ£€æµ‹)
    if Config.COMPRESSION_ENABLED and data.startswith(b'\x1f\x8b'):
        return gzip.decompress(data)
    return data


def calculate_hash(content: bytes, use_blake2b: bool = True) -> tuple[str, str]:
    """
    ğŸ” è®¡ç®—æ•°æ®å“ˆå¸Œ

    ä½¿ç”¨ blake2b æˆ– MD5 ç®—æ³•è®¡ç®—å†…å®¹çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ–‡ä»¶å»é‡

    Args:
        content: å¾…è®¡ç®—çš„å­—èŠ‚æ•°æ®
        use_blake2b: æ˜¯å¦ä½¿ç”¨ blake2bï¼ˆé»˜è®¤ Trueï¼‰ï¼ŒFalse åˆ™ä½¿ç”¨ MD5

    Returns:
        tuple[str, str]: (å“ˆå¸Œå€¼, å“ˆå¸Œç®—æ³•æ ‡è¯† "blake2b" æˆ– "md5")

    æ³¨æ„:
        - blake2b æ¯” MD5 æ›´å¿«ä¸”æ›´å®‰å…¨
        - digest_size=16 ç”Ÿæˆ 128 ä½ï¼ˆ32 ä½åå…­è¿›åˆ¶ï¼‰ï¼Œä¸ MD5 é•¿åº¦ç›¸åŒ
        - ç›¸åŒå†…å®¹å¿…ç„¶äº§ç”Ÿç›¸åŒå“ˆå¸Œï¼Œå®ç°"ç§’ä¼ "åŠŸèƒ½
    """
    if use_blake2b:
        # blake2b digest_size=16 ç”Ÿæˆ 128 ä½ï¼ˆ32 ä½åå…­è¿›åˆ¶ï¼‰ï¼Œä¸ MD5 é•¿åº¦ä¸€è‡´
        return hashlib.blake2b(content, digest_size=16).hexdigest(), "blake2b"
    else:
        return hashlib.md5(content).hexdigest(), "md5"


def validate_and_minify(content: bytes) -> bytes:
    """
    âœ… æ ¡éªŒå¹¶å‹ç¼© JSON

    ä½¿ç”¨ orjson æ ¡éªŒ JSON æ ¼å¼ï¼Œå¹¶å»é™¤å¤šä½™ç©ºæ ¼

    Args:
        content: å¾…æ ¡éªŒçš„ JSON å­—èŠ‚æ•°æ®

    Returns:
        bytes: å‹ç¼©åçš„ JSON å­—èŠ‚æ•°æ® (æ— ç©ºæ ¼ã€æ— æ¢è¡Œ)

    Raises:
        HTTPException: JSON æ ¼å¼æ— æ•ˆã€è¿‡å¤§ã€åµŒå¥—è¿‡æ·±æ—¶æŠ›å‡º 400 é”™è¯¯

    æ³¨æ„:
        - orjson æ¯” stdlib json å¿« 5-10 å€
        - å¼ºåˆ¶æ ¡éªŒç¡®ä¿å­˜å‚¨çš„éƒ½æ˜¯åˆæ³• JSON
        - éªŒè¯æ·±åº¦ã€å­—æ®µæ•°é‡å’Œæ€»å¤§å°ï¼Œé˜²æ­¢æ¶æ„æ”»å‡»
    """
    config = JSONValidationConfig()

    # å…ˆæ£€æŸ¥æ€»å¤§å°
    if len(content) > config.max_total_length:
        raise HTTPException(
            status_code=413,
            detail=f"ğŸ“„ JSON è¿‡å¤§ï¼ˆæœ€å¤§ {config.max_total_length // 1024 // 1024} MBï¼‰"
        )

    try:
        # è§£æ JSON (åŒæ—¶æ ¡éªŒæ ¼å¼)
        obj = orjson.loads(content)

        # éªŒè¯ JSON ç»“æ„ (æ·±åº¦ã€å­—æ®µæ•°é‡)
        _validate_json_structure(obj, config=config)

        # åºåˆ—åŒ–å› JSON (å»é™¤ç©ºæ ¼å’Œæ¢è¡Œ)
        return orjson.dumps(obj)

    except orjson.JSONDecodeError:
        # JSON æ ¼å¼æ— æ•ˆ
        raise HTTPException(status_code=400, detail="ğŸ“„ JSON æ ¼å¼æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹")
    except HTTPException:
        # é‡æ–°æŠ›å‡ºæˆ‘ä»¬çš„éªŒè¯é”™è¯¯
        raise


def calculate_expiry(limit: TimeLimit) -> datetime.datetime | None:
    """
    ğŸ“… è®¡ç®—è¿‡æœŸæ—¶é—´

    æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„æœ‰æ•ˆæœŸè®¡ç®—å…·ä½“çš„è¿‡æœŸæ—¶é—´ç‚¹

    Args:
        limit: æœ‰æ•ˆæœŸæšä¸¾ (1å¤©/7å¤©/1æœˆ/æ°¸ä¹…)

    Returns:
        datetime | None: è¿‡æœŸæ—¶é—´ç‚¹ï¼Œæ°¸ä¹…è¿”å› None

    æ³¨æ„:
        - æ—¶é—´ä»å½“å‰æ—¶åˆ»å¼€å§‹è®¡ç®—
        - 1 æœˆæŒ‰ 30 å¤©è®¡ç®—
    """
    if limit == TimeLimit.PERMANENT:
        return None
    # å¤©æ•°æ˜ å°„
    days_map = {
        TimeLimit.ONE_DAY: 1,
        TimeLimit.SEVEN_DAYS: 7,
        TimeLimit.ONE_MONTH: 30
    }
    return datetime.datetime.now() + datetime.timedelta(days=days_map.get(limit, 0))


# ==========================================
# ğŸ“¤ æ–‡ä»¶ä¸Šä¼ å¤„ç†
# ==========================================

async def process_file_upload(file: UploadFile, time_limit: TimeLimit):
    """
    ğŸ“¤ å¤„ç†æ–‡ä»¶ä¸Šä¼ 

    å®Œæ•´çš„ä¸Šä¼ å¤„ç†æµç¨‹:
        1. æ–‡ä»¶å¤§å°æ£€æŸ¥
        2. åç¼€åæ ¡éªŒ
        3. è¯»å–å¹¶æ ‡å‡†åŒ– JSON
        4. å“ˆå¸ŒæŸ¥é‡ (ç§’ä¼ )
        5. æ•°æ®å‹ç¼© (å¯é€‰)
        6. æ•°æ®åŠ å¯† (å¯é€‰)
        7. æœ¬åœ°å­˜å‚¨
        8. OSS å­˜å‚¨ (å¯é€‰)
        9. å†™å…¥å…ƒæ•°æ®

    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
        time_limit: æ–‡ä»¶æœ‰æ•ˆæœŸ (1å¤©/7å¤©/1æœˆ/æ°¸ä¹…)

    Returns:
        dict: åŒ…å« url, filename, expiry, is_duplicate çš„å“åº”å­—å…¸

    Raises:
        HTTPException: æ–‡ä»¶è¿‡å¤§ã€æ ¼å¼é”™è¯¯ç­‰å¼‚å¸¸
    """

    # ========== 1. æ–‡ä»¶å¤§å°æ£€æŸ¥ ==========
    # è¯»å–æ–‡ä»¶å†…å®¹åˆ°å†…å­˜ (å°æ–‡ä»¶åœºæ™¯)
    raw_content = await file.read()

    file_size = len(raw_content)
    if file_size > Config.MAX_FILE_SIZE:
        log.warning(f"ğŸ“¦ æ–‡ä»¶è¿‡å¤§: {file_size} å­—èŠ‚ï¼Œé™åˆ¶: {Config.MAX_FILE_SIZE} å­—èŠ‚")
        raise HTTPException(
            status_code=413,
            detail=f"ğŸ“¦ æ–‡ä»¶è¿‡å¤§ï¼Œé™åˆ¶ä¸º {Config.MAX_FILE_SIZE} å­—èŠ‚"
        )

    log.info(f"ğŸ“¦ æ¥æ”¶æ–‡ä»¶: {file.filename} ({file_size} å­—èŠ‚)")

    # ========== 2. åç¼€åæ ¡éªŒ ==========
    ext = Path(file.filename).suffix.lower()
    if ext not in Config.ALLOWED_EXTENSIONS:
        log.warning(f"ğŸš« ä¸å…è®¸çš„æ–‡ä»¶ç±»å‹: {ext}")
        raise HTTPException(
            status_code=400,
            detail=f"ğŸš« ä¸å…è®¸çš„æ–‡ä»¶ç±»å‹ï¼Œä»…æ”¯æŒ: {', '.join(Config.ALLOWED_EXTENSIONS)}"
        )

    # ========== 3. JSON æ ¡éªŒå¹¶æ ‡å‡†åŒ– ==========
    try:
        minified_content = validate_and_minify(raw_content)
        log.info(f"âœ… JSON æ ¡éªŒé€šè¿‡ï¼Œå‹ç¼©å: {len(minified_content)} å­—èŠ‚")
    except HTTPException:
        raise

    # ========== 4. å“ˆå¸ŒæŸ¥é‡ ==========
    file_hash, hash_algorithm = calculate_hash(minified_content, use_blake2b=True)

    conn = await get_db_connection()
    # æŸ¥è¯¢æ˜¯å¦å­˜åœ¨ç›¸åŒå“ˆå¸Œçš„æ–‡ä»¶ï¼ˆåŒæ—¶æ”¯æŒ blake2b å’Œ md5ï¼‰
    cursor = await conn.execute("""
        SELECT id, oss_path FROM files
        WHERE (file_hash = ? AND hash_algorithm = 'blake2b')
           OR (file_hash = ? AND hash_algorithm = 'md5')
    """, (file_hash, file_hash))
    existing = await cursor.fetchone()

    if existing:
        # å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›ç°æœ‰é“¾æ¥ (ç§’ä¼ )
        log.info(f"âœ¨ æ£€æµ‹åˆ°é‡å¤æ–‡ä»¶ï¼Œä½¿ç”¨ç§’ä¼ : {file_hash}")
        await conn.close()

        # åŠ å¯†/å‹ç¼©æ¨¡å¼ä¸‹ç»Ÿä¸€è¿”å› API é“¾æ¥
        if Config.ENCRYPTION_ENABLED or Config.COMPRESSION_ENABLED:
            return_url = f"{Config.HOST_DOMAIN}/f/{existing['id']}"
        else:
            # æ˜æ–‡æ¨¡å¼ä¼˜å…ˆè¿”å› OSS é“¾æ¥
            return_url = existing['oss_path'] if existing['oss_path'] else f"{Config.HOST_DOMAIN}/f/{existing['id']}"

        return {
            "url": return_url,
            "filename": file.filename,
            "is_duplicate": True,
            "expiry": "æ°¸ä¹…"
        }

    # ========== 5. æ•°æ®å¤„ç† (å‹ç¼© -> åŠ å¯†) ==========
    # 5.1 å‹ç¼© (å¯é€‰)
    processed_content = compress_data(minified_content)
    if Config.COMPRESSION_ENABLED:
        compression_ratio = len(processed_content) / len(minified_content)
        log.info(f"ğŸ—œï¸ å‹ç¼©å®Œæˆ: å‹ç¼©ç‡ {compression_ratio:.1%}")

    # 5.2 åŠ å¯† (å¯é€‰)
    final_content = CryptoEngine.encrypt(processed_content)

    # ========== 6. æ–‡ä»¶å­˜å‚¨ ==========
    # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶ ID (8 ä½åå…­è¿›åˆ¶ï¼Œä½¿ç”¨å®‰å…¨çš„éšæœºæ•°)
    file_id = secrets.token_hex(4)

    # ç¡®å®šå­˜å‚¨æ–‡ä»¶å
    # åŠ å¯†/å‹ç¼©æ¨¡å¼ä¸‹ä½¿ç”¨ .bin åç¼€ï¼Œé¿å…è¯¯å¯¼
    if Config.ENCRYPTION_ENABLED or Config.COMPRESSION_ENABLED:
        save_filename = f"{file_id}.bin"
    else:
        save_filename = f"{file_id}{ext}"

    # 6.1 æœ¬åœ°å­˜å‚¨
    local_path = Path(Config.UPLOAD_DIR) / save_filename
    async with await anyio.open_file(str(local_path), 'wb') as f:
        await f.write(final_content)
    log.info(f"ğŸ’¾ æœ¬åœ°å­˜å‚¨å®Œæˆ: {save_filename}")

    # 6.2 OSS å­˜å‚¨ (å¯é€‰)
    oss_url = None
    if Config.ENABLE_OSS:
        # ä½¿ç”¨ OSS å®¢æˆ·ç«¯ä¸Šä¼ 
        from app.core.oss_client import OSSClient
        try:
            oss_url = await OSSClient.upload(save_filename, final_content)
            log.info(f"â˜ï¸ OSS ä¸Šä¼ æˆåŠŸ: {oss_url}")
        except Exception as e:
            log.error(f"â˜ï¸ OSS ä¸Šä¼ å¤±è´¥: {e}")
            # OSS ä¸Šä¼ å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œä»ä½¿ç”¨æœ¬åœ°å­˜å‚¨

    # ========== 7. ç”Ÿæˆè¿”å›é“¾æ¥ ==========
    if Config.ENCRYPTION_ENABLED or Config.COMPRESSION_ENABLED:
        # åŠ å¯†/å‹ç¼©æ¨¡å¼å¿…é¡»èµ° API è§£å¯†
        return_url = f"{Config.HOST_DOMAIN}/f/{file_id}"
    else:
        # æ˜æ–‡æ¨¡å¼ä¼˜å…ˆè¿”å› OSS é“¾æ¥
        return_url = oss_url if oss_url else f"{Config.HOST_DOMAIN}/f/{file_id}"

    # ========== 8. å†™å…¥å…ƒæ•°æ® ==========
    expire_at = calculate_expiry(time_limit)

    try:
        await conn.execute("""
            INSERT INTO files (id, file_hash, hash_algorithm, filename, local_path, oss_path, expire_at)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (file_id, file_hash, hash_algorithm, file.filename, save_filename, oss_url, expire_at))
        await conn.commit()
    except Exception as e:
        log.error(f"ğŸ’¥ æ•°æ®åº“å†™å…¥å¤±è´¥: {e}")
        raise e
    finally:
        await conn.close()

    log.info(f"âœ… ä¸Šä¼ æˆåŠŸ: {file_id} -> {return_url}")

    return {
        "url": return_url,
        "filename": file.filename,
        "expiry": str(expire_at) if expire_at else "æ°¸ä¹…",
        "is_duplicate": False
    }


# ==========================================
# ğŸ“¥ æ–‡ä»¶è¯»å–å¤„ç†
# ==========================================

async def retrieve_file_content(file_id: str):
    """
    ğŸ“¥ è·å–æ–‡ä»¶å†…å®¹

    å®Œæ•´çš„è¯»å–å¤„ç†æµç¨‹:
        1. æŸ¥è¯¢æ•°æ®åº“è·å–æ–‡ä»¶è·¯å¾„
        2. è¯»å–æœ¬åœ°æ–‡ä»¶
        3. è§£å¯† (å¦‚æœåŠ å¯†)
        4. è§£å‹ (å¦‚æœå‹ç¼©)
        5. è¿”å›åŸå§‹ JSON

    Args:
        file_id: æ–‡ä»¶çš„å”¯ä¸€ ID

    Returns:
        tuple: (æ–‡ä»¶å†…å®¹ bytes, åŸå§‹æ–‡ä»¶å str)ï¼Œä¸å­˜åœ¨æ—¶è¿”å› (None, None)

    Raises:
        HTTPException: æ–‡ä»¶æŸåã€è§£å¯†å¤±è´¥ç­‰å¼‚å¸¸
    """

    # ========== 1. æŸ¥è¯¢æ–‡ä»¶å…ƒæ•°æ® ==========
    # å…ˆæ£€æŸ¥ç¼“å­˜
    cached_metadata = _metadata_cache.get(file_id)
    if cached_metadata:
        local_path = Path(Config.UPLOAD_DIR) / cached_metadata["local_path"]
        original_name = cached_metadata["filename"]
    else:
        conn = await get_db_connection()
        cursor = await conn.execute("SELECT local_path, filename FROM files WHERE id = ?", (file_id,))
        row = await cursor.fetchone()
        await conn.close()

        if not row:
            # æ–‡ä»¶ä¸å­˜åœ¨
            log.warning(f"ğŸ” æ–‡ä»¶ä¸å­˜åœ¨: {file_id}")
            return None, None

        local_path = Path(Config.UPLOAD_DIR) / row['local_path']
        original_name = row['filename']
        # å†™å…¥ç¼“å­˜
        _metadata_cache[file_id] = {"local_path": row['local_path'], "filename": original_name}

    # ========== 2. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ ==========
    if not local_path.exists():
        log.warning(f"ğŸ” æ–‡ä»¶å·²ä¸¢å¤±: {local_path}ï¼Œæ¸…ç†æ•°æ®åº“è®°å½•")
        # æ–‡ä»¶ä¸¢å¤±ï¼Œæ¸…ç†æ•°æ®åº“è®°å½•
        conn = await get_db_connection()
        await conn.execute("DELETE FROM files WHERE id = ?", (file_id,))
        await conn.commit()
        await conn.close()
        invalidate_file_cache(file_id)
        return None, None

    # ========== 3. è¯»å–æ–‡ä»¶å†…å®¹ ==========
    try:
        async with await anyio.open_file(str(local_path), 'rb') as f:
            content = await f.read()
    except Exception as e:
        log.error(f"ğŸ’¥ æ–‡ä»¶è¯»å–å¤±è´¥ {file_id}: {e}")
        raise HTTPException(status_code=500, detail="ğŸ“„ æ–‡ä»¶è¯»å–å¤±è´¥")

    # ========== 4. é€†å‘å¤„ç† (è§£å¯† -> è§£å‹) ==========
    try:
        # 4.1 è§£å¯† (å¦‚æœåŠ å¯†)
        decrypted = CryptoEngine.decrypt(content)

        # 4.2 è§£å‹ (å¦‚æœå‹ç¼©)
        final_json = decompress_data(decrypted)

        return final_json, original_name

    except Exception as e:
        log.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥ {file_id}: {e}")
        raise HTTPException(
            status_code=500,
            detail="ğŸ“„ æ–‡ä»¶æŸåæˆ–è§£å¯†å¤±è´¥"
        )


# ==========================================
# ğŸ§¹ åå°æ¸…ç†ä»»åŠ¡
# ==========================================

async def clean_expired_task():
    """
    ğŸ§¹ åå°æ¸…ç†è¿‡æœŸæ–‡ä»¶ä»»åŠ¡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

    åŠŸèƒ½:
        - å®šæœŸæ‰«ææ•°æ®åº“ä¸­çš„è¿‡æœŸæ–‡ä»¶
        - æ‰¹é‡åˆ é™¤æœ¬åœ°æ–‡ä»¶
        - æ‰¹é‡åˆ é™¤ OSS æ–‡ä»¶ (å¦‚æœå¯ç”¨)
        - æ‰¹é‡åˆ é™¤æ•°æ®åº“è®°å½•

    è¿è¡Œå‘¨æœŸ:
        - æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ (3600 ç§’)

    æ³¨æ„:
        - è¿™æ˜¯ä¸€ä¸ªæ— é™å¾ªç¯çš„ä»»åŠ¡ï¼Œåœ¨åº”ç”¨å¯åŠ¨æ—¶åˆ›å»º
        - å¼‚å¸¸ä¼šè¢«æ•è·å¹¶è®°å½•ï¼Œä¸ä¼šä¸­æ–­ä»»åŠ¡å¾ªç¯
        - ä½¿ç”¨æ‰¹é‡æ“ä½œå’Œå¹¶å‘å¤„ç†æå‡æ€§èƒ½
    """

    log.info("ğŸ§¹ åå°æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡")

    # æ‰¹é‡å¤§å°
    BATCH_SIZE = 100

    while True:
        try:
            # ========== 1. åˆ†æ‰¹æŸ¥è¯¢è¿‡æœŸæ–‡ä»¶ ==========
            conn = await get_db_connection()
            now = datetime.datetime.now()

            # åˆ†æ‰¹æŸ¥è¯¢è¿‡æœŸæ–‡ä»¶
            cursor = await conn.execute(
                "SELECT id, local_path, oss_path FROM files WHERE expire_at < ? LIMIT ?",
                (now, BATCH_SIZE)
            )
            rows = await cursor.fetchall()

            if not rows:
                await conn.close()
            else:
                log.info(f"ğŸ§¹ å‘ç° {len(rows)} ä¸ªè¿‡æœŸæ–‡ä»¶éœ€è¦æ¸…ç†")

                # ========== 2. æ”¶é›†éœ€è¦åˆ é™¤çš„æ–‡ä»¶ä¿¡æ¯ ==========
                to_delete_local = []
                to_delete_oss = []
                file_ids = []

                for row in rows:
                    file_ids.append(row['id'])
                    local_full = Path(Config.UPLOAD_DIR) / row['local_path']
                    to_delete_local.append(str(local_full))
                    if row['oss_path']:
                        to_delete_oss.append(row['oss_path'])

                # ========== 3. å¹¶å‘åˆ é™¤æœ¬åœ°æ–‡ä»¶ ==========
                async def delete_local(path: str):
                    path_obj = Path(path)
                    if path_obj.exists():
                        try:
                            await asyncio.to_thread(path_obj.unlink)
                            return path, True
                        except OSError as e:
                            log.error(f"âš ï¸ åˆ é™¤æœ¬åœ°æ–‡ä»¶å¤±è´¥ {path}: {e}")
                            return path, False
                    return path, False

                local_results = await asyncio.gather(
                    *[delete_local(p) for p in to_delete_local],
                    return_exceptions=True
                )

                deleted_count = sum(1 for r in local_results if isinstance(r, tuple) and r[1])
                log.info(f"ğŸ—‘ï¸ æ¸…ç†ä»»åŠ¡: å·²åˆ é™¤ {deleted_count}/{len(to_delete_local)} ä¸ªæœ¬åœ°æ–‡ä»¶")

                # ========== 4. æ‰¹é‡åˆ é™¤ OSS æ–‡ä»¶ ==========
                if to_delete_oss and Config.ENABLE_OSS:
                    from app.core.oss_client import OSSClient
                    for oss_url in to_delete_oss:
                        try:
                            await OSSClient.delete(oss_url)
                            log.info(f"â˜ï¸ æ¸…ç†ä»»åŠ¡: å·²åˆ é™¤ OSS æ–‡ä»¶ {oss_url}")
                        except Exception as e:
                            log.error(f"âš ï¸ åˆ é™¤ OSS æ–‡ä»¶å¤±è´¥ {oss_url}: {e}")

                # ========== 5. æ‰¹é‡åˆ é™¤æ•°æ®åº“è®°å½•ï¼ˆå•æ¬¡äº‹åŠ¡ï¼‰==========
                placeholders = ','.join('?' * len(file_ids))
                await conn.execute(
                    f"DELETE FROM files WHERE id IN ({placeholders})",
                    file_ids
                )
                await conn.commit()
                await conn.close()

                # æ¸…é™¤ç¼“å­˜
                for file_id in file_ids:
                    invalidate_file_cache(file_id)

                log.info(f"âœ… æ¸…ç†ä»»åŠ¡å®Œæˆï¼Œå…±æ¸…ç† {len(file_ids)} ä¸ªæ–‡ä»¶")

                # ========== 6. ç»§ç»­æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤š ==========
                if len(rows) == BATCH_SIZE:
                    continue

        except Exception as e:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œé˜²æ­¢ä»»åŠ¡å¾ªç¯ä¸­æ–­
            log.error(f"ğŸš¨ æ¸…ç†ä»»åŠ¡ä¸¥é‡é”™è¯¯: {e}")

        # ========== 7. ç­‰å¾…ä¸‹æ¬¡æ‰§è¡Œ ==========
        # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ (3600 ç§’)
        await asyncio.sleep(3600)


# ==========================================
# ğŸ“‹ ç®¡ç†åå°ä¸šåŠ¡é€»è¾‘
# ==========================================

async def get_file_list(
    page: int = 1,
    page_size: int = 20,
    search: str = "",
    sort: str = "created_at",
    order: str = "desc"
) -> dict:
    """
    ğŸ“‹ è·å–æ–‡ä»¶åˆ—è¡¨

    Args:
        page: é¡µç ï¼ˆä» 1 å¼€å§‹ï¼‰
        page_size: æ¯é¡µå¤§å°
        search: æœç´¢å…³é”®è¯ï¼ˆæ–‡ä»¶åæˆ– IDï¼‰
        sort: æ’åºå­—æ®µ
        order: æ’åºæ–¹å‘ï¼ˆasc/descï¼‰

    Returns:
        dict: åŒ…å« items, total, page, page_size, total_pages çš„å­—å…¸
    """
    conn = await get_db_connection()

    # æ„å»º WHERE æ¡ä»¶
    where_conditions = []
    params = []
    if search:
        where_conditions.append("(filename LIKE ? OR id LIKE ?)")
        params.extend([f"%{search}%", f"%{search}%"])

    where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"

    # è·å–æ€»æ•°
    count_query = f"SELECT COUNT(*) as count FROM files WHERE {where_clause}"
    cursor = await conn.execute(count_query, params)
    total_row = await cursor.fetchone()
    total = total_row['count'] if total_row else 0

    # è®¡ç®—åç§»é‡
    offset = (page - 1) * page_size

    # æ„å»ºæ’åº
    order_clause = f"{sort} {order.upper()}"

    # è·å–æ–‡ä»¶åˆ—è¡¨
    now = datetime.datetime.now()
    list_query = f"""
        SELECT id, filename, file_hash, local_path, oss_path, expire_at, created_at
        FROM files
        WHERE {where_clause}
        ORDER BY {order_clause}
        LIMIT ? OFFSET ?
    """
    cursor = await conn.execute(list_query, params + [page_size, offset])
    rows = await cursor.fetchall()
    await conn.close()

    # æ„å»ºç»“æœ
    items = []
    for row in rows:
        # è·å–æ–‡ä»¶å¤§å°
        file_size = 0
        local_path = Path(Config.UPLOAD_DIR) / row['local_path']
        if local_path.exists():
            file_size = local_path.stat().st_size

        # åˆ¤æ–­æ˜¯å¦è¿‡æœŸ (SQLite è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è½¬æ¢ä¸º datetime)
        is_expired = False
        if row['expire_at']:
            expire_at = datetime.datetime.fromisoformat(row['expire_at']) if isinstance(row['expire_at'], str) else row['expire_at']
            is_expired = expire_at < now

        items.append({
            "id": row['id'],
            "filename": row['filename'],
            "file_hash": row['file_hash'],
            "local_path": row['local_path'],
            "oss_path": row['oss_path'],
            # SQLite å·²è¿”å› ISO æ ¼å¼å­—ç¬¦ä¸²ï¼Œæ— éœ€è°ƒç”¨ isoformat()
            "expire_at": row['expire_at'],
            "created_at": row['created_at'],
            "file_size": file_size,
            "is_expired": is_expired
        })

    total_pages = (total + page_size - 1) // page_size

    return {
        "items": items,
        "total": total,
        "page": page,
        "page_size": page_size,
        "total_pages": total_pages
    }


async def get_file_detail(file_id: str) -> dict | None:
    """
    ğŸ“„ è·å–æ–‡ä»¶è¯¦æƒ…

    Args:
        file_id: æ–‡ä»¶ ID

    Returns:
        dict | None: æ–‡ä»¶è¯¦æƒ…ï¼Œä¸å­˜åœ¨æ—¶è¿”å› None
    """
    conn = await get_db_connection()
    cursor = await conn.execute(
        "SELECT * FROM files WHERE id = ?",
        (file_id,)
    )
    row = await cursor.fetchone()
    await conn.close()

    if not row:
        return None

    # è·å–æ–‡ä»¶å¤§å°
    file_size = 0
    local_path = Path(Config.UPLOAD_DIR) / row['local_path']
    if local_path.exists():
        file_size = local_path.stat().st_size

    # è·å–æ–‡ä»¶å†…å®¹
    content = None
    content_bytes, filename = await retrieve_file_content(file_id)
    if content_bytes:
        try:
            content = content_bytes.decode('utf-8')
        except:
            content = None

    return {
        "id": row['id'],
        "filename": row['filename'],
        "file_hash": row['file_hash'],
        "hash_algorithm": row['hash_algorithm'],
        "local_path": row['local_path'],
        "oss_path": row['oss_path'],
        # SQLite å·²è¿”å› ISO æ ¼å¼å­—ç¬¦ä¸²ï¼Œæ— éœ€è°ƒç”¨ isoformat()
        "expire_at": row['expire_at'],
        "created_at": row['created_at'],
        "file_size": file_size,
        "content": content
    }


async def delete_file(file_id: str) -> bool:
    """
    ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶

    Args:
        file_id: æ–‡ä»¶ ID

    Returns:
        bool: æ˜¯å¦åˆ é™¤æˆåŠŸ
    """
    conn = await get_db_connection()

    # è·å–æ–‡ä»¶ä¿¡æ¯
    cursor = await conn.execute("SELECT local_path, oss_path FROM files WHERE id = ?", (file_id,))
    row = await cursor.fetchone()

    if not row:
        await conn.close()
        return False

    # åˆ é™¤æœ¬åœ°æ–‡ä»¶
    local_path = Path(Config.UPLOAD_DIR) / row['local_path']
    if local_path.exists():
        try:
            await asyncio.to_thread(local_path.unlink)
        except Exception as e:
            log.error(f"åˆ é™¤æœ¬åœ°æ–‡ä»¶å¤±è´¥ {local_path}: {e}")

    # åˆ é™¤ OSS æ–‡ä»¶
    if row['oss_path'] and Config.ENABLE_OSS:
        from app.core.oss_client import OSSClient
        try:
            await OSSClient.delete(row['oss_path'])
        except Exception as e:
            log.error(f"åˆ é™¤ OSS æ–‡ä»¶å¤±è´¥ {row['oss_path']}: {e}")

    # åˆ é™¤æ•°æ®åº“è®°å½•
    await conn.execute("DELETE FROM files WHERE id = ?", (file_id,))
    await conn.commit()
    await conn.close()

    # æ¸…é™¤ç¼“å­˜
    invalidate_file_cache(file_id)

    return True


async def batch_delete_files(file_ids: list[str]) -> dict:
    """
    ğŸ—‘ï¸ æ‰¹é‡åˆ é™¤æ–‡ä»¶

    Args:
        file_ids: æ–‡ä»¶ ID åˆ—è¡¨

    Returns:
        dict: åŒ…å«æˆåŠŸå’Œå¤±è´¥æ•°é‡çš„å­—å…¸
    """
    success_count = 0
    failed_count = 0

    for file_id in file_ids:
        result = await delete_file(file_id)
        if result:
            success_count += 1
        else:
            failed_count += 1

    return {
        "success": success_count,
        "failed": failed_count
    }


async def get_storage_stats() -> dict:
    """
    ğŸ“Š è·å–å­˜å‚¨ç»Ÿè®¡

    Returns:
        dict: å­˜å‚¨ç»Ÿè®¡æ•°æ®
    """
    conn = await get_db_connection()

    # æ€»æ–‡ä»¶æ•°å’Œå¤§å°
    cursor = await conn.execute("SELECT COUNT(*) as count FROM files")
    total_row = await cursor.fetchone()
    total_files = total_row['count'] if total_row else 0

    # è®¡ç®—æ€»å­˜å‚¨å¤§å°
    total_size = 0
    by_type = {}
    by_expiry = {"permanent": 0, "1d": 0, "7d": 0, "1m": 0}
    expired_count = 0

    cursor = await conn.execute("SELECT local_path, filename, expire_at FROM files")
    rows = await cursor.fetchall()

    now = datetime.datetime.now()
    upload_dir = Path(Config.UPLOAD_DIR)

    for row in rows:
        # è·å–æ–‡ä»¶å¤§å°
        local_path = upload_dir / row['local_path']
        size = 0
        if local_path.exists():
            size = local_path.stat().st_size
        total_size += size

        # æŒ‰ç±»å‹ç»Ÿè®¡
        ext = Path(row['filename']).suffix.lower() or "æ— åç¼€"
        by_type[ext] = by_type.get(ext, 0) + 1

        # æŒ‰è¿‡æœŸæ—¶é—´ç»Ÿè®¡
        if row['expire_at'] is None:
            by_expiry["permanent"] += 1
        else:
            # è®¡ç®—è¿‡æœŸå¤©æ•° (SQLite è¿”å›å­—ç¬¦ä¸²ï¼Œéœ€è¦è½¬æ¢ä¸º datetime)
            expire_at = datetime.datetime.fromisoformat(row['expire_at']) if isinstance(row['expire_at'], str) else row['expire_at']
            delta = (expire_at - now).days
            if delta < 0:
                expired_count += 1
            elif delta <= 1:
                by_expiry["1d"] += 1
            elif delta <= 7:
                by_expiry["7d"] += 1
            else:
                by_expiry["1m"] += 1

    await conn.close()

    return {
        "total_files": total_files,
        "total_size": total_size,
        "by_type": by_type,
        "by_expiry": by_expiry,
        "expired_count": expired_count
    }


async def get_upload_trend(days: int = 30) -> dict:
    """
    ğŸ“ˆ è·å–ä¸Šä¼ è¶‹åŠ¿

    Args:
        days: ç»Ÿè®¡å¤©æ•°

    Returns:
        dict: åŒ…å« dates, counts, sizes çš„å­—å…¸
    """
    conn = await get_db_connection()

    # è®¡ç®—æ—¥æœŸèŒƒå›´
    end_date = datetime.datetime.now()
    start_date = end_date - datetime.timedelta(days=days)

    # æŸ¥è¯¢æ¯å¤©çš„æ–‡ä»¶æ•°é‡
    cursor = await conn.execute("""
        SELECT
            DATE(created_at) as date,
            COUNT(*) as count
        FROM files
        WHERE created_at >= ?
        GROUP BY DATE(created_at)
        ORDER BY date
    """, (start_date,))

    rows = await cursor.fetchall()
    await conn.close()

    # æ„å»ºå®Œæ•´çš„æ—¥æœŸåºåˆ—
    dates = []
    counts = []
    sizes = []

    for i in range(days):
        date = start_date + datetime.timedelta(days=i)
        date_str = date.strftime("%Y-%m-%d")
        dates.append(date_str)

        # æŸ¥æ‰¾è¯¥æ—¥æœŸçš„è®¡æ•° (SQLite çš„ DATE() å‡½æ•°è¿”å›å­—ç¬¦ä¸²ï¼Œæ— éœ€æ ¼å¼åŒ–)
        count = 0
        for row in rows:
            row_date = row['date'] if row['date'] else ""
            if row_date == date_str:
                count = row['count']
                break
        counts.append(count)
        sizes.append(0)  # æš‚ä¸è¿”å›å¤§å°è¶‹åŠ¿

    return {
        "dates": dates,
        "counts": counts,
        "sizes": sizes
    }


async def get_expiring_files(days: int = 7) -> dict:
    """
    â° è·å–å³å°†è¿‡æœŸçš„æ–‡ä»¶

    Args:
        days: å¤©æ•°èŒƒå›´

    Returns:
        dict: åŒ…å«å³å°†è¿‡æœŸæ–‡ä»¶ä¿¡æ¯çš„å­—å…¸
    """
    conn = await get_db_connection()

    # è®¡ç®—æ—¶é—´èŒƒå›´
    now = datetime.datetime.now()
    end_date = now + datetime.timedelta(days=days)

    # æŸ¥è¯¢å³å°†è¿‡æœŸçš„æ–‡ä»¶
    cursor = await conn.execute("""
        SELECT id, filename, expire_at
        FROM files
        WHERE expire_at IS NOT NULL
            AND expire_at > ?
            AND expire_at <= ?
        ORDER BY expire_at ASC
    """, (now, end_date))

    rows = await cursor.fetchall()
    await conn.close()

    files = []
    for row in rows:
        # SQLite è¿”å›å­—ç¬¦ä¸²ï¼Œéœ€è¦è½¬æ¢ä¸º datetime
        expire_at = datetime.datetime.fromisoformat(row['expire_at']) if isinstance(row['expire_at'], str) else row['expire_at']
        delta = (expire_at - now).days
        files.append({
            "id": row['id'],
            "filename": row['filename'],
            "expire_at": row['expire_at'],  # å·²æ˜¯ ISO æ ¼å¼å­—ç¬¦ä¸²
            "days_until_expiry": max(0, delta)
        })

    return {
        "expiring_soon": len(files),
        "files": files
    }


async def manual_cleanup() -> dict:
    """
    ğŸ§¹ æ‰‹åŠ¨è§¦å‘æ¸…ç†è¿‡æœŸæ–‡ä»¶

    Returns:
        dict: æ¸…ç†ç»“æœ
    """
    conn = await get_db_connection()
    now = datetime.datetime.now()

    # æŸ¥è¯¢è¿‡æœŸæ–‡ä»¶
    cursor = await conn.execute("SELECT id, local_path, oss_path FROM files WHERE expire_at < ?")
    rows = await cursor.fetchall()

    if not rows:
        await conn.close()
        return {"cleaned": 0, "message": "æ²¡æœ‰è¿‡æœŸæ–‡ä»¶éœ€è¦æ¸…ç†"}

    cleaned = 0

    for row in rows:
        file_id = row['id']
        local_path = Path(Config.UPLOAD_DIR) / row['local_path']

        # åˆ é™¤æœ¬åœ°æ–‡ä»¶
        if local_path.exists():
            try:
                await asyncio.to_thread(local_path.unlink)
            except Exception as e:
                log.error(f"åˆ é™¤æœ¬åœ°æ–‡ä»¶å¤±è´¥ {local_path}: {e}")

        # åˆ é™¤ OSS æ–‡ä»¶
        if row['oss_path'] and Config.ENABLE_OSS:
            from app.core.oss_client import OSSClient
            try:
                await OSSClient.delete(row['oss_path'])
            except Exception as e:
                log.error(f"åˆ é™¤ OSS æ–‡ä»¶å¤±è´¥ {row['oss_path']}: {e}")

        # åˆ é™¤æ•°æ®åº“è®°å½•
        await conn.execute("DELETE FROM files WHERE id = ?", (file_id,))
        invalidate_file_cache(file_id)
        cleaned += 1

    await conn.commit()
    await conn.close()

    return {"cleaned": cleaned, "message": f"å·²æ¸…ç† {cleaned} ä¸ªè¿‡æœŸæ–‡ä»¶"}


# ==========================================
# ğŸ‘ï¸ æ–‡ä»¶ç³»ç»Ÿç›‘æ§ä»»åŠ¡
# ==========================================

async def sync_missing_files_task():
    """
    ğŸ‘ï¸ åŒæ­¥ä¸¢å¤±æ–‡ä»¶ä»»åŠ¡

    åŠŸèƒ½:
        - å®šæœŸæ‰«ææ•°æ®åº“ä¸­çš„æ–‡ä»¶è®°å½•
        - æ£€æŸ¥ç£ç›˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        - è‡ªåŠ¨æ¸…ç†ä¸¢å¤±æ–‡ä»¶çš„æ•°æ®åº“è®°å½•

    è¿è¡Œå‘¨æœŸ:
        - æ¯ 30 ç§’æ‰§è¡Œä¸€æ¬¡

    æ³¨æ„:
        - å¤„ç†ç£ç›˜æ–‡ä»¶è¢«ç›´æ¥åˆ é™¤çš„æƒ…å†µ
        - ä¿è¯æ•°æ®åº“ä¸ç£ç›˜çŠ¶æ€ä¸€è‡´
    """
    log.info("ğŸ‘ï¸ æ–‡ä»¶åŒæ­¥ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯ 30 ç§’æ‰§è¡Œä¸€æ¬¡")

    while True:
        try:
            conn = await get_db_connection()

            # æŸ¥è¯¢æ‰€æœ‰æ–‡ä»¶è®°å½•
            cursor = await conn.execute("SELECT id, local_path FROM files")
            rows = await cursor.fetchall()
            await conn.close()

            missing_count = 0
            for row in rows:
                file_id = row['id']
                local_path = Path(Config.UPLOAD_DIR) / row['local_path']

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not local_path.exists():
                    missing_count += 1
                    log.info(f"ğŸ—‘ï¸ å‘ç°ä¸¢å¤±æ–‡ä»¶: {file_id}ï¼Œæ¸…ç†æ•°æ®åº“è®°å½•")
                    conn = await get_db_connection()
                    await conn.execute("DELETE FROM files WHERE id = ?", (file_id,))
                    await conn.commit()
                    await conn.close()
                    invalidate_file_cache(file_id)

            if missing_count > 0:
                log.info(f"âœ… åŒæ­¥ä»»åŠ¡å®Œæˆï¼Œæ¸…ç† {missing_count} ä¸ªä¸¢å¤±æ–‡ä»¶è®°å½•")

        except Exception as e:
            log.error(f"ğŸš¨ æ–‡ä»¶åŒæ­¥ä»»åŠ¡é”™è¯¯: {e}")

        # ç­‰å¾… 30 ç§’åå†æ¬¡æ‰§è¡Œ
        await asyncio.sleep(30)


# ==========================================
# ğŸ“Š Prometheus æŒ‡æ ‡è§£æ
# ==========================================

# ç¼“å­˜ Prometheus æŒ‡æ ‡ç»“æœï¼ˆ10ç§’è¿‡æœŸï¼‰
_metrics_cache: TTLCache = TTLCache(maxsize=1, ttl=10)
_metrics_cache_time: float = 0
_startup_time: float = time.time()


def _parse_prometheus_labels(labels_str: str) -> dict:
    """
    è§£æ Prometheus æ ‡ç­¾å­—ä¸²

    Args:
        labels_str: æ ‡ç­¾å­—ä¸²ï¼Œå¦‚ 'method="GET",path="/api"'

    Returns:
        dict: è§£æåçš„æ ‡ç­¾å­—å…¸
    """
    labels = {}
    for match in re.finditer(r'(\w+)="([^"]*)"', labels_str):
        labels[match.group(1)] = match.group(2)
    return labels


async def get_prometheus_metrics() -> dict:
    """
    ğŸ“Š è·å– Prometheus ç›‘æ§æŒ‡æ ‡ï¼ˆJSON æ ¼å¼ï¼‰

    é€šè¿‡è®¿é—® /metrics ç«¯ç‚¹è·å– Prometheus æ ¼å¼æ•°æ®ï¼Œ
    è§£æåè¿”å›å‰ç«¯å¯ç”¨çš„ JSON ç»“æ„ã€‚

    Returns:
        dict: åŒ…å« requests, latency, errors, system çš„æŒ‡æ ‡å­—å…¸

    æŒ‡æ ‡è¯´æ˜:
        - requests: è¯·æ±‚ç»Ÿè®¡ï¼ˆæ€»æ•°ã€QPSã€æŒ‰æ–¹æ³•/è·¯å¾„åˆ†ç»„ï¼‰
        - latency: å»¶è¿Ÿç»Ÿè®¡ï¼ˆp50/p90/p95/p99 å¹³å‡ï¼‰
        - errors: é”™è¯¯ç»Ÿè®¡ï¼ˆæ€»æ•°ã€é”™è¯¯ç‡ã€æŒ‰çŠ¶æ€ç åˆ†ç»„ï¼‰
        - system: ç³»ç»ŸæŒ‡æ ‡ï¼ˆè¿è¡Œæ—¶é•¿ã€å†…å­˜ä½¿ç”¨ï¼‰
    """
    global _metrics_cache_time

    current_time = time.time()
    if current_time - _metrics_cache_time < 10 and _metrics_cache:
        return _metrics_cache

    import httpx

    result = {
        "requests": {
            "total": 0,
            "qps": 0,
            "by_method": {},
            "by_path": {}
        },
        "latency": {
            "p50": 0,
            "p90": 0,
            "p95": 0,
            "p99": 0,
            "avg": 0
        },
        "errors": {
            "total": 0,
            "rate": 0,
            "by_status": {}
        },
        "system": {
            "uptime": int(current_time - _startup_time),
            "memory_usage": 0,
            "total_memory": 0,
            "cpu_usage": 0
        }
    }

    try:
        # è®¿é—®æœ¬åœ° metrics ç«¯ç‚¹
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get("http://localhost:8000/metrics")
            metrics_text = response.text
    except Exception as e:
        log.warning(f"ğŸ“Š è·å– Prometheus æŒ‡æ ‡å¤±è´¥: {e}")
        return result

    # ========== è§£æ http_server_requests_count ==========
    total_requests = 0
    status_counts = {}

    for match in re.finditer(
        r'http_server_requests_count\{([^}]*)\} (\d+)',
        metrics_text
    ):
        labels_str = match.group(1)
        value = int(match.group(2))
        labels = _parse_prometheus_labels(labels_str)

        method = labels.get("method", "UNKNOWN")
        path = labels.get("path", "")
        status = labels.get("status_code", "")

        total_requests += value

        # æŒ‰æ–¹æ³•åˆ†ç»„
        if method:
            result["requests"]["by_method"][method] = \
                result["requests"]["by_method"].get(method, 0) + value

        # æŒ‰è·¯å¾„åˆ†ç»„ï¼ˆåªç»Ÿè®¡å‰ 10 ä¸ªï¼‰
        if path and len(result["requests"]["by_path"]) < 10:
            result["requests"]["by_path"][path] = \
                result["requests"]["by_path"].get(path, 0) + value

        # æŒ‰çŠ¶æ€ç åˆ†ç»„
        if status:
            status_counts[status] = status_counts.get(status, 0) + value
            # 4xx å’Œ 5xx è§†ä¸ºé”™è¯¯
            if status.startswith(("4", "5")):
                result["errors"]["total"] += value
                result["errors"]["by_status"][status] = \
                    result["errors"]["by_status"].get(status, 0) + value

    result["requests"]["total"] = total_requests

    # è®¡ç®— QPSï¼ˆåŸºäºè¿è¡Œæ—¶é•¿ï¼‰
    uptime = current_time - _startup_time
    if uptime > 0:
        result["requests"]["qps"] = round(total_requests / uptime, 2)

    # è®¡ç®—é”™è¯¯ç‡
    if total_requests > 0:
        result["errors"]["rate"] = round(
            (result["errors"]["total"] / total_requests) * 100, 2
        )

    # ========== è§£æ http_server_requests_duration_seconds_bucket ==========
    # è§£æå»¶è¿Ÿç›´æ–¹å›¾æ•°æ®
    latency_buckets: dict[str, list[float]] = {}

    for match in re.finditer(
        r'http_server_requests_duration_seconds_bucket\{([^}]*)\} (\d+)',
        metrics_text
    ):
        labels_str = match.group(1)
        value = int(match.group(2))
        labels = _parse_prometheus_labels(labels_str)

        le = labels.get("le", "")
        if le == "+Inf":
            continue

        if le not in latency_buckets:
            latency_buckets[le] = []
        latency_buckets[le].append(value)

    # è®¡ç®—åˆ†ä½æ•°ï¼ˆåŸºäºæ‰€æœ‰è·¯å¾„çš„æ•°æ®ï¼‰
    if latency_buckets:
        # è·å–æ‰€æœ‰æ¡¶ä¸­çš„æœ€å¤§å€¼ï¼ˆæ€»é‡ï¼‰
        bucket_values = []
        for le in sorted(latency_buckets.keys(), key=float):
            if latency_buckets[le]:
                bucket_values.append(max(latency_buckets[le]))

        if bucket_values:
            total_samples = max(bucket_values) if bucket_values else 1

            # ä¼°ç®—åˆ†ä½æ•°ï¼ˆåŸºäº Prometheus çš„æ¡¶åˆ†å¸ƒï¼‰
            # p50: 0.1s, p90: 0.5s, p95: 0.75s, p99: 1s
            percentile_map = {"0.1": "p50", "0.5": "p90", "0.75": "p95", "1": "p99"}
            for le_str, key in percentile_map.items():
                # æ‰¾åˆ°å¯¹åº”çš„æ¡¶
                for le in latency_buckets:
                    if float(le) <= float(le_str) and latency_buckets[le]:
                        ratio = max(latency_buckets[le]) / total_samples if total_samples > 0 else 0
                        if ratio >= 0.5:
                            result["latency"][key] = int(float(le) * 1000)
                            break

            # è®¡ç®—å¹³å‡å»¶è¿Ÿï¼ˆä» _sum å’Œ _count æŒ‡æ ‡ï¼‰
            sum_match = re.search(
                r'http_server_requests_duration_seconds_sum\{[^}]*\} ([\d.]+)',
                metrics_text
            )
            count_match = re.search(
                r'http_server_requests_duration_seconds_count\{[^}]*\} (\d+)',
                metrics_text
            )
            if sum_match and count_match:
                total_sum = float(sum_match.group(1))
                total_count = int(count_match.group(1))
                if total_count > 0:
                    result["latency"]["avg"] = int((total_sum / total_count) * 1000)

    # ========== ç³»ç»ŸæŒ‡æ ‡ ==========
    try:
        # å†…å­˜ä¿¡æ¯ï¼ˆç³»ç»Ÿçº§ï¼‰
        mem = psutil.virtual_memory()
        # å·²ç”¨å†…å­˜ï¼ˆMBï¼‰
        result["system"]["memory_usage"] = round((mem.total - mem.available) / 1024 / 1024, 2)
        # å†…å­˜æ€»é‡ï¼ˆMBï¼‰
        result["system"]["total_memory"] = round(mem.total / 1024 / 1024, 2)

        # CPU ä½¿ç”¨ç‡ï¼ˆç³»ç»Ÿçº§ï¼Œç™¾åˆ†æ¯”ï¼‰
        result["system"]["cpu_usage"] = round(psutil.cpu_percent(interval=0.1), 2)
    except Exception as e:
        log.warning(f"è·å–ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")

    # æ›´æ–°ç¼“å­˜
    _metrics_cache.clear()
    _metrics_cache.update(result)
    _metrics_cache_time = current_time

    return result
